import argparse
import cv2
import torch
import os
from torchvision import models, transforms
from tqdm import tqdm
from attacks.ALA_lib import RGB2Lab_t, Lab2RGB_t, light_filter, Normalize, update_paras
from log_config import logger


class attack_ALA_classification:
    def __init__(self, model, config):
        self.model = model
        self.config = config
        self.device = config.device
        self.batch_size = config.batch_size
        self.segment = config.segment
        self.init_range = config.init_range
        self.transform = config.transform
        self.tau = config.tau
        self.eta = config.eta
        self.lr = config.lr

    def attack(self, image, epsilon, label, **kwargs):
        # logger.debug(f'before attack image shape: {image.shape}')

        # ä»[1, 3, 32. 32] tensor -> [32, 32, 3] tensor
        perturbed_image = image.clone().detach().to(self.device)
        perturbed_image = perturbed_image.squeeze(0)
        perturbed_image = perturbed_image.permute(1, 2, 0)
        # logger.debug(f'before attack  perturbed_image shape: {perturbed_image.shape}, type:{type(perturbed_image)}')
        # å°†è¾“å‡ºå›¾åƒä»[0,1]è½¬åŒ–ä¸º[0,255]
        perturbed_image = perturbed_image * 255

        # å°†RGBå›¾åƒè½¬æ¢åˆ°Labé¢œè‰²ç©ºé—´å¹¶å½’ä¸€åŒ–
        X_ori = (RGB2Lab_t(perturbed_image / 1.0) + 128) / 255.0
        # åœ¨ç¬¬ä¸€ä¸ªä½ç½®æ·»åŠ ä¸€ä¸ªç»´åº¦ [L, A, B]->[1, L, A, B]
        X_ori = X_ori.unsqueeze(0).type(torch.FloatTensor).to(self.device)
        best_adversary = perturbed_image.clone().to(self.device)
        # å°†å¼ é‡è½¬æ¢ä¸º PIL å›¾åƒï¼Œæ–¹ä¾¿åç»­çš„å¯è§†åŒ–æˆ–ä¿å­˜æ“ä½œ
        # mid_image = transforms.ToPILImage()(perturbed_image.squeeze(0).cpu())

        # åˆ†ç¦»Lé€šé“ï¼ˆå…‰åº¦ï¼‰å’Œaã€bé€šé“ï¼ˆé¢œè‰²ï¼‰
        # lightï¼šå½¢çŠ¶ä¸º [batch_size, 1, H, W]ï¼ŒåŒ…å«å…‰åº¦Lé€šé“ã€‚
        # colorï¼šå½¢çŠ¶ä¸º [batch_size, 2, H, W]ï¼ŒåŒ…å«é¢œè‰²aå’Œbé€šé“ã€‚
        light, color = torch.split(X_ori, [1, 2], dim=1)
        # light_maxï¼šå½¢çŠ¶ä¸º [batch_size, 1]ï¼ŒåŒ…å«æ¯ä¸ªæ‰¹æ¬¡å›¾åƒçš„å…‰åº¦Lé€šé“çš„æœ€å¤§å€¼ã€‚
        light_max = torch.max(light, dim=2)[0].max(dim=2)[0]
        # light_minï¼šå½¢çŠ¶ä¸º [batch_size, 1]ï¼ŒåŒ…å«æ¯ä¸ªæ‰¹æ¬¡å›¾åƒçš„å…‰åº¦Lé€šé“çš„æœ€å°å€¼ã€‚
        light_min = torch.min(light, dim=2)[0].min(dim=2)[0]

        color = color.to(self.device)
        light = light.to(self.device)

        # éšæœºåˆå§‹åŒ–
        if self.config.random_init:
            # ä»£è¡¨åœ¨argsä¸­å¯åŠ¨äº†å‚æ•°éšæœºåˆå§‹åŒ–
            # segmentä¸ºåˆ†æ®µæ•°é‡
            # ä¸€å¼€å§‹éšæœºåˆå§‹åŒ–çš„èŒƒå›´ä¸º[0,1]
            Paras_light = torch.rand(self.batch_size, 1, self.segment).to(self.device)
            # åˆå§‹åŒ–èŒƒå›´ä¸º[m,n]
            # init_range[1]ä¸ºn
            # init_range[0]ä¸ºm
            total_range = self.init_range[1] - self.init_range[0]
            # å°†Paras_lightä¸€å¼€å§‹ä»[0,1]çš„èŒƒå›´æ˜ å°„åˆ°[m,n]èŒƒå›´
            Paras_light = Paras_light * total_range + self.init_range[0]
        else:
            Paras_light = torch.ones(self.batch_size, 1, self.segment).to(self.device)
        Paras_light.requires_grad = True

        # è¿­ä»£è¿›è¡Œå¯¹æŠ—æ”»å‡»
        for _ in range(epsilon):
            # ä¿®æ”¹å…‰åº¦å€¼
            X_adv_light = light_filter(light, Paras_light, self.segment, light_max.to(self.device),
                                       light_min.to(self.device))
            # å°†äº®åº¦æ‹¼æ¥ï¼Œé‡æ–°å˜æˆLABå›¾åƒ
            X_adv = torch.cat((X_adv_light, color), dim=1) * 255.0
            # å½¢çŠ¶ä¸º [1, C, H, W] å˜ä¸º [C, H, W]
            X_adv = X_adv.squeeze(0)
            # Lab2RGB_t(X_adv - 128)ï¼šå°†Labå›¾åƒè½¬æ¢å›RGBé¢œè‰²ç©ºé—´ã€‚åœ¨è½¬æ¢ä¹‹å‰ï¼Œå°†Labå›¾åƒå€¼å‡å»128ï¼Œä»¥æ¢å¤åˆ°Labé¢œè‰²ç©ºé—´çš„åŸå§‹èŒƒå›´ã€‚
            X_adv = Lab2RGB_t(X_adv - 128) / 255.0
            X_adv = X_adv.type(torch.FloatTensor).to(self.device)
            # å°†å¼ é‡å›¾åƒè½¬æ¢ä¸ºPILå›¾åƒï¼Œä»¥ä¾¿äºåç»­çš„å›¾åƒå¤„ç†æˆ–å¯è§†åŒ–ã€‚
            mid_image = transforms.ToPILImage()(X_adv)
            # åº”ç”¨é¢„å®šä¹‰çš„å›¾åƒè½¬æ¢ï¼ˆä¾‹å¦‚è°ƒæ•´å¤§å°ã€å½’ä¸€åŒ–ï¼‰å°†PILå›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚
            X_adv = self.transform(mid_image).unsqueeze(0).to(self.device)

            # è®¡ç®—å¯¹æŠ—æŸå¤±
            # ä¹Ÿå°±æ˜¯å…¬å¼ä¸­çš„Lc&w
            # åˆ é™¤å½’ä¸€åŒ–
            # logits = self.model(self.norm(X_adv))
            logits = self.model.predict(X_adv)
            # è·å–çœŸå®ç±»åˆ«çš„å¾—åˆ†
            real = logits.gather(1, label.unsqueeze(1)).squeeze(1)
            # logger.debug(f'logits shape: {logits.shape}, real shape: {real.shape}')
            # é™¤çœŸå®ç±»åˆ«å¤–çš„æœ€é«˜å¾—åˆ†
            other = (logits - torch.zeros_like(logits).scatter_(1, label.unsqueeze(1), float('inf'))).max(1)[0]
            adv_loss = torch.clamp(real - other, min=self.tau).sum()

            # å…‰åº¦åˆ†å¸ƒçº¦æŸæŸå¤±
            # å…¬å¼ä¸­çš„æ­£åˆ™åŒ–é¡¹
            paras_loss = 1 - torch.abs(Paras_light).sum() / self.segment
            # æ­£åˆ™åŒ–é¡¹æƒé‡ğ›½
            factor = self.eta
            loss = adv_loss + factor * paras_loss
            loss.backward(retain_graph=True)

            """
            adv_loss: å¯¹æŠ—æŸå¤± ç›®æ ‡æ˜¯ä½¿å¯¹æŠ—æ ·æœ¬çš„é¢„æµ‹ç»“æœåç¦»çœŸå®æ ‡ç­¾ï¼Œä¹Ÿå°±ç±»ä¼¼äºCWä¸­ä¸¤ä¸ªæ ‡ç­¾çš„å·®å€¼
            paras_loss: å‚æ•°æŸå¤± ä¹Ÿå°±æ˜¯æ­£åˆ™åŒ–é¡¹
            loss: æ€»æŸå¤±
            """


            # æ›´æ–°å‚æ•°
            update_paras(Paras_light, self.lr, self.batch_size)

            # é¢„æµ‹å¯¹æŠ—æ ·æœ¬çš„åˆ†ç±»
            x_result = X_adv.detach().clone()
            # è·å–æ ·æœ¬åˆ†ç±»
            predicted_classes = self.model.predict(x_result).argmax(1)
            # å¸ƒå°”å€¼ï¼Œåˆ¤æ–­æ˜¯å¦è¢«é”™è¯¯åˆ†ç±»
            # Trueä»£è¡¨é”™è¯¯åˆ†ç±»
            is_adv = (predicted_classes != label)
        if epsilon != 0:
            return x_result
        else:
            perturbed_image = perturbed_image.permute(2, 0, 1)
            perturbed_image = perturbed_image.unsqueeze(0)
            return perturbed_image
