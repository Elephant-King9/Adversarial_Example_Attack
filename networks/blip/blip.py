"""
 * Copyright (c) 2022, salesforce.com, inc.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause
 * By Junnan Li
"""
import warnings

warnings.filterwarnings("ignore")

from .vit import VisionTransformer, interpolate_pos_embed
from .med import BertConfig, BertModel, BertLMHeadModel
from transformers import BertTokenizer
import os

os.environ['HTTP_PROXY'] = "http://192.168.1.10:7890"
os.environ['HTTPS_PROXY'] = "http://192.168.1.10:7890"

import torch
from torch import nn

import os
from urllib.parse import urlparse
from timm.models.hub import download_cached_file

# è‡ªå®šä¹‰ä½ç½®çš„bertåˆ†è¯å™¨é¢„è®­ç»ƒåœ°å€
bert_tokenizer_path = "././assets/Pre-training_files/BERT/bert-base-uncased"


# ç¼–ç å™¨éƒ¨åˆ†
class BLIP_Base(nn.Module):
    # åˆå§‹åŒ–
    def __init__(self,
                 # æ¨å¸ˆå§è¿™é‡Œçš„configè·¯å¾„åº”è¯¥æ˜¯åœ¨ä½¿ç”¨çš„æ—¶å€™ä¼ å…¥çš„ï¼Œä¸æ˜¯é»˜è®¤çš„è·¯å¾„
                 # æºç é‡Œæ˜¯è¿™ä¸ªè·¯å¾„ï¼Œä½†æ˜¯æ¨å¸ˆå§ä»£ç é‡Œæ˜¯å§med_config.jsonæ–‡ä»¶æ”¾åœ¨äº†åŒç›®å½•ä¸‹ï¼Œæˆ‘è¿™é‡ŒæŒ‰ç…§æºç ä¸ºå‡†
                 med_config='configs/med_config.json',
                 image_size=224,
                 vit='base',
                 vit_grad_ckpt=False,
                 vit_ckpt_layer=0,
                 ):
        """
        Args:
            med_config (str): path for the mixture of encoder-decoder model's configuration file
            image_size (int): input image size
            vit (str): model size of vision transformer
        """
        super().__init__()
        # åˆ›å»ºVitæ¨¡å‹
        self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer)
        # åˆå§‹åŒ–BERTï¼Œå¹¶ä¸”è·å¾—tokenizer(åˆ†è¯å™¨)
        self.tokenizer = init_tokenizer()
        # ä½œç”¨æ˜¯ä»ä¸€ä¸ª med_config(JSON) æ–‡ä»¶ä¸­åŠ è½½ BERT æ¨¡å‹çš„é…ç½®å‚æ•°ï¼Œå¹¶å®ä¾‹åŒ–ä¸€ä¸ª BertConfig å¯¹è±¡
        # med_config æ˜¯ä¼ å…¥çš„å‚æ•°ï¼Œä¸ºJSONæ–‡ä»¶çš„è·¯å¾„
        med_config = BertConfig.from_json_file(med_config)
        # vision_widthä¸ºcreate_vitè¿”å›çš„å‚æ•°ï¼Œå°†å¯¹è±¡ä¸­çš„encoder_widthè®¾ä¸ºè·å–Vitæ¨¡å‹çš„vision_width
        med_config.encoder_width = vision_width
        # BertModelæ˜¯è‡ªå®šä¹‰çš„ç±»(med.py)ï¼Œæ˜¯ä¸€ä¸ªåŸºç¡€çš„ BERT æ¨¡å‹ï¼ŒåªåŒ…å«äº† BERT çš„ç¼–ç å™¨éƒ¨åˆ†
        # åˆå§‹åŒ–ä¸€ä¸ªBERTæ¨¡å‹ï¼Œä½¿ç”¨med_configé…ç½®ï¼Œä¸æ·»åŠ æ± åŒ–å±‚

        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)

    def forward(self, image, caption, mode):
        """

        :param image: Tensorå›¾ç‰‡
        :param caption:è¡¨ç¤ºæ–‡æœ¬è¾“å…¥çš„å­—ç¬¦ä¸²ã€‚
        :param mode:æŒ‡å®šæ“ä½œæ¨¡å¼çš„å­—ç¬¦ä¸²ï¼ˆ'image', 'text', æˆ– 'multimodal'ï¼‰
        :return:è¡¨ç¤ºç»™å®šæ¨¡å¼çš„è¾“å‡ºç‰¹å¾çš„å¼ é‡
        """
        # è¾“å…¥çš„æ¨¡å‹åªèƒ½æ˜¯: å›¾åƒã€æ–‡å­—ã€å¤šæ¨¡æ€
        assert mode in ['image', 'text', 'multimodal'], "mode parameter must be image, text, or multimodal"

        # å°†captionè¾“å…¥åˆ†è¯å™¨è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶ä¸”è¿”å›å€¼ä¸ºtensorå¼ é‡
        # return_tensors="pt" ä»£è¡¨è¿”å›Pytorch Tensorå¼ é‡
        # textä»£è¡¨å­—å…¸ç”Ÿæˆè¿™ä¸ªå¥å­çš„ç¼–ç çŸ©é˜µ
        text = self.tokenizer(caption, return_tensors="pt").to(image.device)

        if mode == 'image':
            # å°†å›¾åƒé€šè¿‡Vitç¼–ç å™¨è½¬åŒ–ä¸ºimage_embeds
            image_embeds = self.visual_encoder(image)
            return image_embeds

        elif mode == 'text':
            # input_idså°†åˆ†è¯å™¨(tokenizer)æ‹†åˆ†æˆçš„æ›´å°çš„å•å…ƒ(token)æ˜ å°„åˆ°å”¯ä¸€ä¸€ä¸ªç´¢å¼•
            # attention_mask æ˜¯ç”¨äºåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­æŒ‡ç¤ºå“ªäº› token éœ€è¦è¢«æ³¨æ„çš„æ©ç ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒç”¨äºåŒºåˆ†å®é™…çš„æ–‡æœ¬ token å’Œå¡«å……çš„ tokenï¼ˆpadding tokenï¼‰
            # ä½¿ç”¨ return_dict=True å¯ä»¥æ›´æ–¹ä¾¿åœ°è®¿é—®ç¼–ç å™¨çš„å¤šä¸ªè¾“å‡º
            # mode='text' è¿™ä¸ªå‚æ•°æŒ‡å®šç¼–ç å™¨çš„æ¨¡å¼ä¸ºæ–‡æœ¬æ¨¡å¼ã€‚
            text_output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask,
                                            return_dict=True, mode='text')
            return text_output.last_hidden_state

        elif mode == 'multimodal':
            # è·å–å›¾åƒEmbedding
            image_embeds = self.visual_encoder(image)
            # åˆå§‹åŒ–Attentionä¸­çš„ğ›¼
            image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)
            # æ¯ä¸ªåºåˆ—çš„ç¬¬ä¸€ä¸ª token è®¾ç½®ä¸ºä¸€ä¸ªç‰¹å®šçš„ç¼–ç å™¨ token
            # ç±»ä¼¼äºç”Ÿæˆ[CLS]?
            text.input_ids[:, 0] = self.tokenizer.enc_token_id
            # å°†å›¾åƒæ–‡æœ¬ç»“åˆè¿›è¡Œç¼–ç 
            output = self.text_encoder(text.input_ids,
                                       attention_mask=text.attention_mask,
                                       encoder_hidden_states=image_embeds,
                                       encoder_attention_mask=image_atts,
                                       return_dict=True,
                                       )
            # å®ƒä»£è¡¨äº†ç¼–ç å™¨åœ¨å¤„ç†å®Œè¾“å…¥åºåˆ—ï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œå›¾åƒç‰¹å¾ï¼‰åçš„æœ€åéšè—çŠ¶æ€
            return output.last_hidden_state


class BLIP_Decoder(nn.Module):
    def __init__(self,
                 med_config='configs/med_config.json',
                 image_size=384,
                 vit='base',
                 vit_grad_ckpt=False,
                 vit_ckpt_layer=0,
                 prompt='a picture of ',
                 ):
        # ä¸BLIP_BaseåŸºæœ¬ç›¸åŒï¼Œä¿®æ”¹äº†text_encoder->text_decoderï¼Œå¤šäº†promptï¼Œä½œç”¨ä¼šåœ¨ä»£ç ä¸­çš„æ³¨é‡Šè®²è§£
        """
        Args:
            med_config (str): path for the mixture of encoder-decoder model's configuration file
            image_size (int): input image size
            vit (str): model size of vision transformer
        """
        super().__init__()
        # åˆ›å»ºæ–¹æ³•ç›¸åŒï¼Œä¹Ÿæ˜¯è¿”å›Encoder
        self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer)
        # åŒBaseï¼Œåˆ›å»ºåˆ†è¯å™¨
        self.tokenizer = init_tokenizer()
        # åŒBaseï¼Œé…ç½®æ–‡ä»¶
        med_config = BertConfig.from_json_file(med_config)
        # åŒBaseï¼Œè®¾ç½®encoderç»´åº¦
        med_config.encoder_width = vision_width
        # ä¸Baseä¸åŒï¼Œè°ƒç”¨çš„æ–¹æ³•ä¸åŒ
        # BertLMHeadModel åœ¨ BertModel çš„åŸºç¡€ä¸Šæ·»åŠ äº†è¯­è¨€æ¨¡å‹å¤´ï¼Œä¸“ç”¨äºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œé€‚åˆç”Ÿæˆæˆ–è¡¥å…¨æ–‡æœ¬
        self.text_decoder = BertLMHeadModel(config=med_config)

        # promptçš„ä½œç”¨æ˜¯å¼•å¯¼æ–‡æœ¬ç”Ÿæˆï¼Œå…ˆç»™æ–‡æœ¬ä¸€ä¸ªå¼€å¤´ï¼Œè®©æ–‡æœ¬ç”Ÿæˆå¯ä»¥æ ¹æ®è®¤ä¸ºæä¾›çš„å¼€å¤´æ¥æ›´å¥½çš„ç”Ÿæˆåç»­æ–‡æœ¬
        self.prompt = prompt
        # è°ƒç”¨åˆ†è¯å™¨ï¼ˆtokenizerï¼‰å°† prompt è½¬æ¢ä¸º token IDs
        # input_ids ç”¨äºè·å¾—æ–‡å­—æ ¹æ®å­—å…¸çš„ç¼–ç åºåˆ—ï¼ŒEg. [101, 7592, 1010, 2129, 2024, 2017, 102]
        # å‡å» 1 é€šå¸¸æ˜¯ä¸ºäº†æ’é™¤ç‰¹æ®Š token çš„å½±å“ï¼Œä¾‹å¦‚ [CLS] æˆ– [SEP] token
        self.prompt_length = len(self.tokenizer(self.prompt).input_ids) - 1

    def forward(self, image, caption):
        # ä¸ç”¨åŒºåˆ†ä¸‰ç§æƒ…å†µäº†
        image_embeds = self.visual_encoder(image)
        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)

        text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors="pt").to(
            image.device)

        text.input_ids[:, 0] = self.tokenizer.bos_token_id

        decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)
        decoder_targets[:, :self.prompt_length] = -100

        decoder_output = self.text_decoder(text.input_ids,
                                           attention_mask=text.attention_mask,
                                           encoder_hidden_states=image_embeds,
                                           encoder_attention_mask=image_atts,
                                           labels=decoder_targets,
                                           return_dict=True,
                                           )
        loss_lm = decoder_output.loss

        return loss_lm

    def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9,
                 repetition_penalty=1.0):
        image_embeds = self.visual_encoder(image)

        if not sample:
            image_embeds = image_embeds.repeat_interleave(num_beams, dim=0)

        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)
        model_kwargs = {"encoder_hidden_states": image_embeds, "encoder_attention_mask": image_atts}

        prompt = [self.prompt] * image.size(0)
        input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to(image.device)
        input_ids[:, 0] = self.tokenizer.bos_token_id
        input_ids = input_ids[:, :-1]

        if sample:
            # nucleus sampling
            outputs = self.text_decoder.generate(input_ids=input_ids,
                                                 max_length=max_length,
                                                 min_length=min_length,
                                                 do_sample=True,
                                                 top_p=top_p,
                                                 num_return_sequences=1,
                                                 eos_token_id=self.tokenizer.sep_token_id,
                                                 pad_token_id=self.tokenizer.pad_token_id,
                                                 repetition_penalty=1.1,
                                                 **model_kwargs)
        else:
            # beam search
            outputs = self.text_decoder.generate(input_ids=input_ids,
                                                 max_length=max_length,
                                                 min_length=min_length,
                                                 num_beams=num_beams,
                                                 eos_token_id=self.tokenizer.sep_token_id,
                                                 pad_token_id=self.tokenizer.pad_token_id,
                                                 repetition_penalty=repetition_penalty,
                                                 **model_kwargs)

        captions = []
        for output in outputs:
            caption = self.tokenizer.decode(output, skip_special_tokens=True)
            captions.append(caption[len(self.prompt):])
        return captions


def blip_decoder(pretrained='', **kwargs):
    model = BLIP_Decoder(**kwargs)
    if pretrained:
        model, msg = load_checkpoint(model, pretrained)
        assert (len(msg.missing_keys) == 0)
    return model


def blip_feature_extractor(pretrained='', **kwargs):
    model = BLIP_Base(**kwargs)
    if pretrained:
        model, msg = load_checkpoint(model, pretrained)
        assert (len(msg.missing_keys) == 0)
    return model


# ç”¨äºåˆå§‹åŒ–ä¸€ä¸ª BERT åˆ†è¯å™¨ï¼ˆTokenizerï¼‰ï¼Œå¹¶ä¸ºå…¶æ·»åŠ ä¸€äº›ç‰¹æ®Šçš„æ ‡è®°ï¼ˆtokensï¼‰
def init_tokenizer():
    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ BERT åˆ†è¯å™¨ï¼Œå¡«å…¥æœ¬åœ°çš„æ–‡ä»¶è·¯å¾„
    # è¿™ä¸ªåœ°æ–¹æ˜¯æ¨å¸ˆå§è‡ªå·±é…ç½®çš„è·¯å¾„
    # tokenizer = BertTokenizer.from_pretrained('assets/bert-base-uncased/')
    # è¿™æ˜¯æˆ‘è¦é…ç½®çš„è·¯å¾„
    tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)
    # å¥½åƒæ˜¯bertåˆ†æ®µé“¾æ¥çš„ä¸¤ä¸ªæ ‡è®°
    tokenizer.add_special_tokens({'bos_token': '[DEC]'})
    tokenizer.add_special_tokens({'additional_special_tokens': ['[ENC]']})
    # è·å–DECæ ‡è®°
    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]
    # è¿”å›çš„ç±»å‹ä¸ºtransformers.BertTokenizer
    return tokenizer


# ç”¨äºåˆ›å»º Vision Transformer (ViT) æ¨¡å‹
def create_vit(vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0):
    """
    :param vit: æŒ‡å®šè¦åˆ›å»ºçš„ ViT æ¨¡å‹çš„å¤§å°ï¼Œå¯ä»¥æ˜¯ 'base' æˆ– 'large'
    :param image_size: è¾“å…¥å›¾åƒçš„å°ºå¯¸ã€‚
    :param use_grad_checkpointing: æ˜¯å¦ä½¿ç”¨æ¸è¿›å±‚æ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œä»¥èŠ‚çœå†…å­˜ï¼ˆé»˜è®¤å€¼ä¸º Falseï¼‰
    :param ckpt_layer: æŒ‡å®šä»å“ªä¸ªå±‚å¼€å§‹ä½¿ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹ï¼ˆé»˜è®¤å€¼ä¸º 0ï¼‰
    :param drop_path_rate: éšæœºè·¯å¾„ä¸¢å¼ƒçš„æ¯”ä¾‹ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆé»˜è®¤å€¼ä¸º 0ï¼‰
    :return:
    """

    # æ–­è¨€è¯­å¥ï¼Œç”¨äºåˆ¤æ–­vitå¿…é¡»åœ¨è¿™ä¸¤ä¸ªå‚æ•°ä¹‹é—´é€‰æ‹©
    assert vit in ['base', 'large'], "vit parameter must be base or large"

    # åˆ›å»ºbaseå¤§å°çš„vit
    if vit == 'base':
        vision_width = 768
        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12,
                                           num_heads=12, use_grad_checkpointing=use_grad_checkpointing,
                                           ckpt_layer=ckpt_layer,
                                           drop_path_rate=0 or drop_path_rate
                                           )
    # åˆ›å»ºlargeå¤§å°çš„vit
    elif vit == 'large':
        vision_width = 1024
        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24,
                                           num_heads=16, use_grad_checkpointing=use_grad_checkpointing,
                                           ckpt_layer=ckpt_layer,
                                           drop_path_rate=0.1 or drop_path_rate
                                           )
    # è¿”å›Encoder: å…·ä½“çš„è§†è§‰ç¼–ç å™¨æ¨¡å‹
    # è¿”å›vision_width:è½¬æ¢ä¸ºå‘é‡çš„ç»´åº¦å¤§å°
    return visual_encoder, vision_width


def is_url(url_or_filename):
    parsed = urlparse(url_or_filename)
    return parsed.scheme in ("http", "https")


def load_checkpoint(model, url_or_filename):
    if is_url(url_or_filename):
        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)
        checkpoint = torch.load(cached_file, map_location='cpu')
    elif os.path.isfile(url_or_filename):
        checkpoint = torch.load(url_or_filename, map_location='cpu')
    else:
        raise RuntimeError('checkpoint url or path is invalid')

    state_dict = checkpoint['model']

    state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],
                                                                   model.visual_encoder)
    if 'visual_encoder_m.pos_embed' in model.state_dict().keys():
        state_dict['visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],
                                                                         model.visual_encoder_m)
    for key in model.state_dict().keys():
        if key in state_dict.keys():
            if state_dict[key].shape != model.state_dict()[key].shape:
                del state_dict[key]

    msg = model.load_state_dict(state_dict, strict=False)
    print('load checkpoint from %s' % url_or_filename)
    return model, msg
